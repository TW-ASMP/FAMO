{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b575de8-5857-4356-8e1c-019bdad5e15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9d4fbb-3b32-4970-a4e4-fea6deb4fb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the roles and sub-roles\n",
    "def subrolegen(num_levels, role, num_subroles, roles, subroles, levels):\n",
    "    if num_levels!=0:\n",
    "        for subrole in range (1,num_subroles + 1):\n",
    "            role = f\"{role}\"\n",
    "            roles.append(role)\n",
    "            subrole = f\"{role}_{subrole}\"\n",
    "            subroles.append(subrole)\n",
    "            levels.append(num_levels)\n",
    "            subrolegen(num_levels-1, subrole, num_subroles, roles, subroles, levels)\n",
    "    return(roles, subroles, levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebf9492-9074-4570-83d2-1d8d7dd7294a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# role hierarchy dataset\n",
    "def role_hierarchy(num_levels,num_sub_roles, prefix=\"\"):\n",
    "    # Create empty lists to store the roles and sub-roles\n",
    "    roles = []\n",
    "    sub_roles = []\n",
    "    levels=[]\n",
    "    \n",
    "    # Generate the roles and sub-roles for all but the last level\n",
    "    (roles_out, subroles_out, levels_out) = (subrolegen(num_levels-1,1,num_sub_roles,roles, sub_roles, levels))\n",
    "    \n",
    "    # Create a dictionary with the roles and sub-roles\n",
    "    data = {\"Role\": roles_out, \"Subrole\": subroles_out, \"Level\": levels_out}\n",
    "\n",
    "    # Create the pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    #reverse levels\n",
    "    df['Level'] = num_levels - df['Level']\n",
    "    \n",
    "    #add sub-roles for all level 8 roles\n",
    "    total_last_level_subroles = 200000 - len(df)\n",
    "    \n",
    "    #count of all level 8 roles\n",
    "    secondlast_roles = df.loc[df['Level']==(num_levels-1)]['Subrole']\n",
    "    last_num_roles = total_last_level_subroles / len(secondlast_roles)\n",
    "    \n",
    "    lastroles=[]\n",
    "    last_levels=[]\n",
    "    for r2 in secondlast_roles:\n",
    "        for i in range (1, round(last_num_roles)+1):\n",
    "            lastroles.append(f\"{r2}\")\n",
    "            last_levels.append(num_levels)\n",
    "            \n",
    "    # Create a dictionary with the roles (no sub-roles) for last level\n",
    "    lastlevel_data = {\"Role\": lastroles, \"Level\": last_levels}\n",
    "\n",
    "    # Create the pandas DataFrame\n",
    "    lastlevel_df = pd.DataFrame(lastlevel_data) \n",
    "    \n",
    "    #append to main dataframe\n",
    "    df=df.append(lastlevel_df)\n",
    "    \n",
    "    #add prefix to role and subrole ID\n",
    "    df['Role'] = prefix + df['Role']\n",
    "    df['Subrole'] = prefix + df['Subrole']\n",
    "    \n",
    "    #create lists of fake attributes\n",
    "    fake = Faker()\n",
    "    fake_enum_list = []\n",
    "    fake_description_list = []\n",
    "    fake_role_status_list = []\n",
    "    fake_criticality_list = []\n",
    "    for i in (range (1, len(df.index)+1)):\n",
    "        enum=f\"{fake.ssn()}\"\n",
    "        fake_enum_list.append(enum)\n",
    "        desc=f\"{fake.paragraph(nb_sentences= 1)}\"\n",
    "        fake_description_list.append(desc)\n",
    "        role_status=f\"{fake.word(ext_word_list=['Active','Deprecated','Suspended','Planned'])}\"\n",
    "        fake_role_status_list.append(role_status)\n",
    "        criticality = random.randint(1, 10)\n",
    "        fake_criticality_list.append(criticality)\n",
    "    #add as new attributes in df    \n",
    "    df['EntityNum'] = fake_enum_list\n",
    "    df['Description'] = fake_description_list\n",
    "    df['RoleStatus'] = fake_role_status_list\n",
    "    df['Criticality'] = fake_criticality_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b53b5fb-597f-42bf-b67c-942646294975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spatial hierarchy dataset\n",
    "#not an ideal structure of data but helpful to sort data to create and connect to other associated sources per TH spec of test data\n",
    "def spatial_hierarchy(facilities, facility_areas, buildings, levels, rooms, interior_areas):\n",
    "    #generate data\n",
    "    data=[]\n",
    "    for facility in facilities:\n",
    "        for area in facility_areas:\n",
    "            data.append([facility, facility + area, facility, \"Facility\"])\n",
    "            for building in buildings: \n",
    "                data.append([facility + area, facility + area + building, facility, \"FacilityArea\"])\n",
    "                for level in levels:\n",
    "                    data.append([facility + area + building,facility + area + building + level, facility, \"Building\"])\n",
    "                    for room in rooms:\n",
    "                        data.append([facility + area + building + level, facility + area + building + level + room, facility, \"Level\"])\n",
    "                        for int_area in int_areas:\n",
    "                            data.append([facility + area + building + level + room, facility + area + building + level + room + int_area, facility, \"Room\"])\n",
    "                            data.append([facility + area + building + level + room + int_area, None, facility, \"InteriorArea\"])                            \n",
    "    df_space = pd.DataFrame(data, columns=['Space', 'ContainsSpace', 'Facility', 'SpaceType'])\n",
    "    \n",
    "    #fake attributes\n",
    "    fake = Faker()\n",
    "    fake_spacenum_list=[]\n",
    "    fake_spacedesc_list=[]\n",
    "    for i in range(0, len(df_space.index)):\n",
    "        spacenum=f\"{fake.ean(length=8)}\"\n",
    "        fake_spacenum_list.append(spacenum)\n",
    "        spacedesc=f\"{fake.paragraph(nb_sentences= 1)}\"\n",
    "        fake_spacedesc_list.append(spacedesc)\n",
    "    df_space['SpaceNum']=fake_spacenum_list\n",
    "    df_space['Description']=fake_spacedesc_list\n",
    "    return df_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1996d7-55f8-4896-933e-a37f4d1e8482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#asset dataset\n",
    "def asset_builder(uid,num_assets,special_class,num_special_class,special_property):\n",
    "    fake = Faker()\n",
    "    asset_list=[]\n",
    "    assetnum_list=[]\n",
    "    serial_num_list=[]\n",
    "    asset_class_list=[]\n",
    "    source_list=[]\n",
    "    state_list=[]\n",
    "    motorsize_list=[]\n",
    "\n",
    "    #Asset: procedurally generated ID\n",
    "    #Asset number: bothify (unique)\n",
    "    #Serialnum: bothify (unique)\n",
    "    #Asset class list: \"http://ontology.eil.utoronto.ca/FAMO/assets/\" + random word\n",
    "    #Source list: from set (WMS or GIS)\n",
    "    #State list: from set\n",
    "\n",
    "    #Later - overwrite num_special_class Asset class randomly as special_class\n",
    "    #Later: Motorsize list: randomly from 10-1500 hp; 5% over 1000 hp\n",
    "    \n",
    "    for i in range(1,num_assets+1):\n",
    "        asset = f\"asset_{uid}_{i}\"\n",
    "        asset_list.append(asset)\n",
    "        assetnum = fake.bothify(text=\"A0####\")\n",
    "        assetnum_list.append(assetnum)\n",
    "        serial_num = fake.bothify(text=\"se-rial-num-ber-####\")\n",
    "        serial_num_list.append(serial_num)\n",
    "        asset_class = f\"http://ontology.eil.utoronto.ca/FAMO/assets/{fake.word(part_of_speech='noun').capitalize()}\"\n",
    "        asset_class_list.append(asset_class)\n",
    "        source = fake.word(ext_word_list=['WMS','GIS'])\n",
    "        source_list.append(source)\n",
    "        state = f\"{fake.word(ext_word_list=['Active Service','Retired','Removed','On Site'])}\"\n",
    "        state_list.append(state)\n",
    "    \n",
    "    num_to_add = num_special_class - asset_class_list.count(special_class)\n",
    "    \n",
    "    #randomly (re-)assign asset class to num_special_class of the asset records\n",
    "    for _ in range (0,num_to_add):\n",
    "        asset_class_list[random.randint(1,len(asset_class_list)+1)] = special_class\n",
    "    # Create a dictionary for the facility 1 assets\n",
    "    asset_data = {\"Asset\": asset_list, \"AssetNum\": assetnum_list, \"SerialNum\": serial_num_list, \"AssetClass\": asset_class_list, \"Source\": source_list, \"State\": state_list}\n",
    "\n",
    "    asset_df = pd.DataFrame(asset_data)\n",
    "    \n",
    "    # add motor size attribute for all motors\n",
    "    #list of random motor sizes:\n",
    "    motorsize_list=[]\n",
    "    for i in range(0,sum(p == special_class for p in asset_class_list)):\n",
    "        motorsize=random.randint(10,1500)\n",
    "        motorsize_list.append(motorsize)\n",
    "    asset_df.loc[asset_df['AssetClass']==special_class, special_property] = motorsize_list\n",
    "\n",
    "    # Create the pandas DataFrame\n",
    "    return asset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39de690-fd04-44ea-b451-04a8e3047148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate spatial data\n",
    "# building blocks for the spatial hierarchy\n",
    "facilities = ['A','B','C','D']\n",
    "facility_areas = ['A1','A2','A3','A4']\n",
    "buildings = ['B1', 'B2', 'B3', 'B4', 'B5']\n",
    "levels = ['l1','l2','l3']\n",
    "rooms = [\"r\" + str(i) for i in range(1, 101)]\n",
    "int_areas = ['ia1', 'ia2', 'ia3', 'ia4']\n",
    "\n",
    "dfspace=spatial_hierarchy(facilities, facility_areas, buildings, levels, rooms, int_areas)\n",
    "\n",
    "#generate role hierarchy with distinct labels (one for each facility)\n",
    "roledf_A=role_hierarchy(9,4,\"role_A_\")\n",
    "roledf_B=role_hierarchy(9,4,\"role_B_\")\n",
    "roledf_C=role_hierarchy(9,4,\"role_C_\")\n",
    "roledf_D=role_hierarchy(9,4,\"role_D_\")\n",
    "\n",
    "#relate roles to spatial data\n",
    "#take n samples from the list of facility A interior areas spatial data with an even distribution where n = the number of level9 roles\n",
    "df_interiorarea_A = (dfspace.loc[(dfspace['Facility']==\"A\") & (dfspace['SpaceType']==\"InteriorArea\")]['Space']).sample(n=(len(roledf_A.loc[roledf_A['Level']==9])), random_state=1, replace=True, ignore_index=True)\n",
    "df_interiorarea_B = (dfspace.loc[(dfspace['Facility']==\"B\") & (dfspace['SpaceType']==\"InteriorArea\")]['Space']).sample(n=(len(roledf_B.loc[roledf_B['Level']==9])), random_state=1, replace=True, ignore_index=True)\n",
    "df_interiorarea_C = (dfspace.loc[(dfspace['Facility']==\"C\") & (dfspace['SpaceType']==\"InteriorArea\")]['Space']).sample(n=(len(roledf_C.loc[roledf_C['Level']==9])), random_state=1, replace=True, ignore_index=True)\n",
    "df_interiorarea_D = (dfspace.loc[(dfspace['Facility']==\"D\") & (dfspace['SpaceType']==\"InteriorArea\")]['Space']).sample(n=(len(roledf_D.loc[roledf_D['Level']==9])), random_state=1, replace=True, ignore_index=True)\n",
    "\n",
    "#add the sampled interior areas to define the Space column for each role dataset\n",
    "roledf_A.loc[roledf_A['Level']==9, \"Space\"] = df_interiorarea_A\n",
    "roledf_B.loc[roledf_B['Level']==9, \"Space\"] = df_interiorarea_B\n",
    "roledf_C.loc[roledf_C['Level']==9, \"Space\"] = df_interiorarea_C\n",
    "roledf_D.loc[roledf_D['Level']==9, \"Space\"] = df_interiorarea_D\n",
    "\n",
    "#generate asset data for each set of role hierarchies\n",
    "assetdf_A = asset_builder('A',len(roledf_A.loc[roledf_A['Level']==9]),\"http://ontology.eil.utoronto.ca/FAMO/assets/Motor\",2000,\"MotorSize\")\n",
    "assetdf_B = asset_builder('B',len(roledf_B.loc[roledf_B['Level']==9]),\"http://ontology.eil.utoronto.ca/FAMO/assets/Motor\",2000,\"MotorSize\")\n",
    "assetdf_C = asset_builder('C',len(roledf_C.loc[roledf_C['Level']==9]),\"http://ontology.eil.utoronto.ca/FAMO/assets/Motor\",2000,\"MotorSize\")\n",
    "assetdf_D = asset_builder('D',len(roledf_D.loc[roledf_D['Level']==9]),\"http://ontology.eil.utoronto.ca/FAMO/assets/Motor\",2000,\"MotorSize\")\n",
    "\n",
    "#relate asset data to roles\n",
    "#add \"serving\" column to associate asset data with (level 9) roles\n",
    "assetdf_A['ServingRole'] = roledf_A.loc[roledf_A['Level']==9, \"Role\"]\n",
    "assetdf_B['ServingRole'] = roledf_B.loc[roledf_B['Level']==9, \"Role\"]\n",
    "assetdf_C['ServingRole'] = roledf_C.loc[roledf_C['Level']==9, \"Role\"]\n",
    "assetdf_D['ServingRole'] = roledf_D.loc[roledf_D['Level']==9, \"Role\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fe16f6-85d8-4a3b-b929-7681b5285a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert all to csv\n",
    "roledf_A.append(roledf_B).append(roledf_C).append(roledf_D).to_csv(\"01-tw-fakeroles.csv\")\n",
    "assetdf_A.append(assetdf_B).append(assetdf_C).append(assetdf_D).to_csv(\"01-tw-fakeassets.csv\")\n",
    "dfspace.to_csv(\"01-tw-fakespaces.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aada12-5c8d-4885-b5be-17641bf906b6",
   "metadata": {},
   "source": [
    "**TODO** \n",
    "* improve fake data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a99c05c-5607-4556-8da9-45de0d1ca5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assetdf_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cb36d3-345f-43c7-892b-fe9e28aaf26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218452"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roledf_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73083715-fb69-491f-a3fa-d37d51b2620c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roledf_A.loc[roledf_A['Level']==9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7dd3514-beab-4efb-8ed7-c8af7e222acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(roledf_A.loc[roledf_A['Level']==9]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb34ea6-319f-4123-93bd-b45f70e59ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
